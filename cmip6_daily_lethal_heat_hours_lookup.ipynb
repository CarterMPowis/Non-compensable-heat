{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048dfcb8-f631-447b-8392-02033c1dbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Estimate the number of lethal heat hours per day in CMIP6 data using the lookup method.\n",
    "\n",
    "Temperature max, min, and RH max and min are assumed to exist on a google bucket.\n",
    "These are pulled down in the analysis.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from lethal_heat import Vecellio22\n",
    "import os.path as path\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.delayed as delayed\n",
    "import dask\n",
    "import glob\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463ec69-dd6f-4bbf-93cd-36c9072eeb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers = 5, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac8caaa-01ad-4212-8ecd-0b859f4d7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_loop(fp_tmax, fp_tmin, fp_rmin, fp_rmean, fp_ii, fp_lookup):\n",
    "    \n",
    "    # Open datasets and extract variables\n",
    "    t_max = xr.open_dataset(fp_tmax)['tasmax'] - 273.15\n",
    "    t_min = xr.open_dataset(fp_tmin)['tasmin'] - 273.15\n",
    "    r_min = xr.open_dataset(fp_rmin)['hursmin']\n",
    "    r_mean = xr.open_dataset(fp_rmean)['hurs']\n",
    "    lookup = xr.open_dataset(fp_lookup)\n",
    "    \n",
    "    # Make output\n",
    "    ds_out = xr.Dataset()\n",
    "    ds_out['lat'] = t_max.lat.values\n",
    "    ds_out['lon'] = t_max.lon.values\n",
    "    ds_out['time'] = t_max.time.values\n",
    "    \n",
    "    # Now just take the chunk for this iteration\n",
    "    t_max = t_max.values\n",
    "    t_min = t_min.values\n",
    "    t_mean = (t_max + t_min) / 2\n",
    "    r_min = r_min.values\n",
    "    r_mean = r_mean.values\n",
    "    \n",
    "    # Make output array\n",
    "    n_t, n_r, n_c = t_max.shape\n",
    "    output = np.zeros_like(t_max)\n",
    "    v22 = Vecellio22(degree=2)\n",
    "    \n",
    "    # Calculate ranges in t and rh at every point.\n",
    "    t_amp = np.abs( t_max - t_min ) / 2\n",
    "    r_amp = np.abs( r_min - r_mean )\n",
    "    \n",
    "    # TOLH Indices for every point in chunk\n",
    "    t_mean_ind = np.round( (t_mean - 25) / .2 + 0.00001 ).astype(int)\n",
    "    r_mean_ind = np.round( (r_mean) / .2 + 0.00001 ).astype(int)\n",
    "    t_amp_ind = np.round( (t_amp) /.5 + 0.00001 ).astype(int)\n",
    "    r_amp_ind = np.round( (r_amp) /.5 + 0.00001 ).astype(int)\n",
    "    \n",
    "    # Clip indices\n",
    "    t_mean_ind = np.clip(t_mean_ind, 0, 77 -1)\n",
    "    r_mean_ind = np.clip(r_mean_ind, 0, 501-1)\n",
    "    t_amp_ind = np.clip(t_amp_ind, 0, 61-1)\n",
    "    r_amp_ind = np.clip(r_amp_ind, 0, 61-1)\n",
    "    \n",
    "    lookup = lookup['hours_over_lh'].values\n",
    "    output = lookup[t_mean_ind, r_mean_ind, t_amp_ind, r_amp_ind]\n",
    "    \n",
    "    ds_out['hours_over_lh'] = (['time','lat','lon'], output)\n",
    "    ds_out.to_netcdf(fp_ii)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba408eb4-6104-45c9-acfb-cbca351340c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'CNRM-CM6-1-HR',\n",
    "          'CNRM-CM6-1', 'CNRM-ESM2-1', 'CanESM5',\n",
    "          'EC-Earth3-Veg-LR', 'FGOALS-g3', 'GFDL-CM4',\n",
    "          'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR',\n",
    "          'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR',\n",
    "          'MPI-ESM1-2-LR', 'MRI-ESM2-0']\n",
    "n_models = len(models)\n",
    "year0 = 1971\n",
    "year1 = 2101\n",
    "yearL = np.arange(year0, year1).astype(int)\n",
    "scenario = 'ssp585'\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        dir_download = '/home/davidbyrne/disks/ssd2/projects/carter2/data/cmip6/'\n",
    "        dir_remote = 'gs://cmip6_data/ISIMIP_BASD_data/daily/{0}/ssp585/'\n",
    "        dir_tmp = '/home/davidbyrne/disks/ssd2/projects/carter2/data/tmp'\n",
    "        fp_out = f'/home/davidbyrne/disks/ssd2/projects/carter2/data/tmp/tolh_{model}_ssp585_{year0}_{year1-1}.nc'\n",
    "        fp_lookup = '/home/davidbyrne/disks/ssd2/projects/carter2/data/tolh_lookup.nc'\n",
    "\n",
    "        fp_tmax_tmp = 'tasmax_{0}_ssp585_basd_0.5deg_{1}.nc'\n",
    "        fp_tmin_tmp = 'tasmin_{0}_ssp585_basd_0.5deg_{1}.nc'\n",
    "        fp_rmin_tmp = 'hursmin_{0}_ssp585_derived_from_basd_data_0.5deg_{1}.nc'\n",
    "        fp_rmean_tmp = 'hurs_{0}_ssp585_basd_0.5deg_{1}.nc'\n",
    "        fp_ii = path.join(dir_tmp, 'tolh_{0}_ssp585_{1}.nc')\n",
    "\n",
    "        fp_tmax_list = [fp_tmax_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "        fp_tmin_list = [fp_tmin_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "        fp_rmin_list = [fp_rmin_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "        fp_rmean_list = [fp_rmean_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "\n",
    "        # GET TEMPERATURE MAX\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'maximum_temperature'\n",
    "        for mm, filename in enumerate(fp_tmax_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # GET TEMPERATURE MIN\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'minimum_temperature'\n",
    "        for mm, filename in enumerate(fp_tmin_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # GET RH MEAN\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'minimum_relative_humidity'\n",
    "        for mm, filename in enumerate(fp_rmin_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # GET RH MIN\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'average_relative_humidity'\n",
    "        for mm, filename in enumerate(fp_rmean_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        n_files = len(fp_tmax_list)\n",
    "        ds_lookup = xr.open_dataset(fp_lookup)\n",
    "        \n",
    "        par_loop_del = delayed(par_loop)\n",
    "        del_list = []\n",
    "\n",
    "        # Loop over delayed list and \n",
    "        for ii in range(n_files):\n",
    "            fp_out = fp_ii.format(model, yearL[ii])\n",
    "            del_list.append( par_loop_del( path.join( dir_download, fp_tmax_list[ii]), \n",
    "                                       path.join( dir_download, fp_tmin_list[ii]) ,\n",
    "                                       path.join( dir_download, fp_rmin_list[ii]), \n",
    "                                       path.join( dir_download, fp_rmean_list[ii]), \n",
    "                                       fp_out, fp_lookup)  )\n",
    "            #par_loop( path.join( dir_download, fp_tmax_list[ii]), \n",
    "            #          path.join( dir_download, fp_tmin_list[ii]) ,\n",
    "            #          path.join( dir_download, fp_rmin_list[ii]), \n",
    "            #          path.join( dir_download, fp_rmean_list[ii]), \n",
    "            #          fp_ii.format(ii), fp_lookup) \n",
    "\n",
    "        # Now concatenate all of the files together\n",
    "        #ds_list = [xr.open_dataset(fp, chunks={'time':150}) for fp in fp_list]\n",
    "        #ds = xr.concat(ds_list, dim='time')\n",
    "        #ds.to_netcdf(fp_out)\n",
    "\n",
    "        dask.compute(*del_list)\n",
    "        \n",
    "        # Move files to google bucket\n",
    "        _ = subprocess.run(f'gsutil -m cp /home/davidbyrne/disks/ssd2/projects/carter2/data/tmp/* gs://fqqzlp/carter2/hours_over_lh/', \n",
    "                           shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # Remove temporary files\n",
    "        _ = subprocess.run(f'rm {dir_tmp}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        _ = subprocess.run(f'rm {dir_download}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "    except:\n",
    "        print(f'Model failed: {model}')\n",
    "        _ = subprocess.run(f'rm {dir_tmp}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        _ = subprocess.run(f'rm {dir_download}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12a84a-51e2-43ee-a934-45270a8acf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
