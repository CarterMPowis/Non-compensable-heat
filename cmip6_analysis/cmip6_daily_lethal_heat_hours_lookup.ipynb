{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048dfcb8-f631-447b-8392-02033c1dbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Estimate the number of lethal heat hours per day in CMIP6 data using the lookup method.\n",
    "\n",
    "Temperature max, min, and RH max and min are assumed to exist on a google bucket.\n",
    "These are pulled down in the analysis. The resulting tolh files (time over lethal heat)\n",
    "are pushed back to the google bucket at the end.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from lethal_heat import Vecellio22\n",
    "import os.path as path\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "import dask.delayed as delayed\n",
    "import dask\n",
    "import glob\n",
    "import subprocess\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463ec69-dd6f-4bbf-93cd-36c9072eeb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dask client for parallel analysis and chunking\n",
    "client = Client(n_workers = 5, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac8caaa-01ad-4212-8ecd-0b859f4d7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_loop(fp_tmax, fp_tmin, fp_rmin, fp_rmean, fp_ii, fp_lookup):\n",
    "    '''\n",
    "    Function to apply within a parallel loop of delayed objects.\n",
    "    '''\n",
    "    \n",
    "    # Open datasets and extract variables\n",
    "    t_max = xr.open_dataset(fp_tmax)['tasmax'] - 273.15\n",
    "    t_min = xr.open_dataset(fp_tmin)['tasmin'] - 273.15\n",
    "    r_min = xr.open_dataset(fp_rmin)['hursmin']\n",
    "    r_mean = xr.open_dataset(fp_rmean)['hurs']\n",
    "    lookup = xr.open_dataset(fp_lookup)\n",
    "    \n",
    "    # Make output\n",
    "    ds_out = xr.Dataset()\n",
    "    ds_out['lat'] = t_max.lat.values\n",
    "    ds_out['lon'] = t_max.lon.values\n",
    "    ds_out['time'] = t_max.time.values\n",
    "    \n",
    "    # Now just take the chunk for this iteration\n",
    "    t_max = t_max.values\n",
    "    t_min = t_min.values\n",
    "    t_mean = (t_max + t_min) / 2\n",
    "    r_min = r_min.values\n",
    "    r_mean = r_mean.values\n",
    "    \n",
    "    # Make output array\n",
    "    n_t, n_r, n_c = t_max.shape\n",
    "    output = np.zeros_like(t_max)\n",
    "    v22 = Vecellio22(degree=2)\n",
    "    \n",
    "    # Calculate ranges in t and rh at every point.\n",
    "    t_amp = np.abs( t_max - t_min ) / 2\n",
    "    r_amp = np.abs( r_min - r_mean )\n",
    "    \n",
    "    # TOLH Indices for every point in chunk\n",
    "    t_mean_ind = np.round( (t_mean - 25) / .2 + 0.00001 ).astype(int)\n",
    "    r_mean_ind = np.round( (r_mean) / .2 + 0.00001 ).astype(int)\n",
    "    t_amp_ind = np.round( (t_amp) /.5 + 0.00001 ).astype(int)\n",
    "    r_amp_ind = np.round( (r_amp) /.5 + 0.00001 ).astype(int)\n",
    "    \n",
    "    # Clip indices\n",
    "    t_mean_ind = np.clip(t_mean_ind, 0, 77 -1)\n",
    "    r_mean_ind = np.clip(r_mean_ind, 0, 501-1)\n",
    "    t_amp_ind = np.clip(t_amp_ind, 0, 61-1)\n",
    "    r_amp_ind = np.clip(r_amp_ind, 0, 61-1)\n",
    "    \n",
    "    lookup = lookup['hours_over_lh'].values\n",
    "    output = lookup[t_mean_ind, r_mean_ind, t_amp_ind, r_amp_ind]\n",
    "    \n",
    "    ds_out['hours_over_lh'] = (['time','lat','lon'], output)\n",
    "    ds_out.to_netcdf(fp_ii)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba408eb4-6104-45c9-acfb-cbca351340c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of model names to find filenames for and analyse\n",
    "models = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'CNRM-CM6-1-HR',\n",
    "          'CNRM-CM6-1', 'CNRM-ESM2-1', 'CanESM5',\n",
    "          'EC-Earth3-Veg-LR', 'FGOALS-g3', 'GFDL-CM4',\n",
    "          'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR',\n",
    "          'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR',\n",
    "          'MPI-ESM1-2-LR', 'MRI-ESM2-0']\n",
    "n_models = len(models)\n",
    "\n",
    "# Define years to analyse\n",
    "year0 = 1971\n",
    "year1 = 2101\n",
    "yearL = np.arange(year0, year1).astype(int)\n",
    "\n",
    "# Analysis scenario (used to find filenames from template)\n",
    "scenario = 'ssp585'\n",
    "\n",
    "# Analyze models one at a time\n",
    "for model in models:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        dir_download = './downloads'\n",
    "        dir_remote = '<remote google bucket directory>'\n",
    "        dir_tmp = '<Directory for temporary files>'\n",
    "        fp_out = f'<output_directory>/tolh_{model}_ssp585_{year0}_{year1-1}.nc'\n",
    "        fp_lookup = './tolh_lookup.nc' # Lookup file from create_lethal_heat_lookup.ipynb\n",
    "\n",
    "        # Filename templates for input datasets (blanks filled in for each loop)\n",
    "        fp_tmax_tmp = 'tasmax_{0}_ssp585_basd_0.5deg_{1}.nc'\n",
    "        fp_tmin_tmp = 'tasmin_{0}_ssp585_basd_0.5deg_{1}.nc'\n",
    "        fp_rmin_tmp = 'hursmin_{0}_ssp585_derived_from_basd_data_0.5deg_{1}.nc'\n",
    "        fp_rmean_tmp = 'hurs_{0}_ssp585_basd_0.5deg_{1}.nc'\n",
    "        fp_ii = path.join(dir_tmp, 'tolh_{0}_ssp585_{1}.nc')\n",
    "\n",
    "        # Get lists of downloaded filenames\n",
    "        fp_tmax_list = [fp_tmax_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "        fp_tmin_list = [fp_tmin_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "        fp_rmin_list = [fp_rmin_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "        fp_rmean_list = [fp_rmean_tmp.format(model, year) for year in np.arange(year0, year1).astype(int)]\n",
    "\n",
    "        # GET TEMPERATURE MAX\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'maximum_temperature'\n",
    "        for mm, filename in enumerate(fp_tmax_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # GET TEMPERATURE MIN\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'minimum_temperature'\n",
    "        for mm, filename in enumerate(fp_tmin_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # GET RH MEAN\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'minimum_relative_humidity'\n",
    "        for mm, filename in enumerate(fp_rmin_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # GET RH MIN\n",
    "        get_cmd = f'gsutil -m cp '\n",
    "        varname = 'average_relative_humidity'\n",
    "        for mm, filename in enumerate(fp_rmean_list):\n",
    "            get_cmd = get_cmd + dir_remote.format(varname) + filename + ' '\n",
    "        get_cmd = get_cmd + dir_download\n",
    "        subprocess.run(get_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # Open lookup file\n",
    "        n_files = len(fp_tmax_list)\n",
    "        ds_lookup = xr.open_dataset(fp_lookup)\n",
    "\n",
    "        # Make delayed version of par_loop\n",
    "        par_loop_del = delayed(par_loop)\n",
    "        del_list = []\n",
    "\n",
    "        # Loop over delayed list and add a delayed instance (looping over annual files)\n",
    "        for ii in range(n_files):\n",
    "            fp_out = fp_ii.format(model, yearL[ii])\n",
    "            del_list.append( par_loop_del( path.join( dir_download, fp_tmax_list[ii]), \n",
    "                                       path.join( dir_download, fp_tmin_list[ii]) ,\n",
    "                                       path.join( dir_download, fp_rmin_list[ii]), \n",
    "                                       path.join( dir_download, fp_rmean_list[ii]), \n",
    "                                       fp_out, fp_lookup)  )\n",
    "\n",
    "        # Compute all for this model in parallel\n",
    "        dask.compute(*del_list)\n",
    "        \n",
    "        # Move files to google bucket\n",
    "        _ = subprocess.run(f'gsutil -m cp {dir_tmp}/* gs://fqqzlp/carter2/hours_over_lh/', \n",
    "                           shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # Remove temporary files\n",
    "        _ = subprocess.run(f'rm {dir_tmp}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        _ = subprocess.run(f'rm {dir_download}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "    except:\n",
    "        print(f'Model failed: {model}')\n",
    "        _ = subprocess.run(f'rm {dir_tmp}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        _ = subprocess.run(f'rm {dir_download}/*', shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12a84a-51e2-43ee-a934-45270a8acf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
